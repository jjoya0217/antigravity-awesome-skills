"""
YouTube ì˜ìƒ ìˆ˜ì§‘ ëª¨ë“ˆ
RSS í”¼ë“œë¥¼ ì‚¬ìš©í•˜ì—¬ ì±„ë„ë³„ ìµœì‹  ì˜ìƒ ëª©ë¡ì„ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
"""
import sys
import os
import re
import xml.etree.ElementTree as ET
from datetime import datetime, timedelta, timezone
from urllib.request import urlopen, Request
from urllib.error import URLError, HTTPError
from typing import List, Dict, Optional

# Windows UTF-8 ì„¤ì •
if sys.platform == "win32":
    sys.stdout.reconfigure(encoding="utf-8", errors="replace")
    sys.stderr.reconfigure(encoding="utf-8", errors="replace")

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ pathì— ì¶”ê°€
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from config import YOUTUBE_CHANNELS, HOURS_LOOKBACK


def get_channel_id_from_handle(handle: str) -> Optional[str]:
    """
    ìœ íŠœë¸Œ ì±„ë„ í•¸ë“¤(@handle)ë¡œë¶€í„° ì±„ë„ IDë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
    ì±„ë„ í˜ì´ì§€ì˜ HTMLì—ì„œ channel_idë¥¼ íŒŒì‹±í•©ë‹ˆë‹¤.
    """
    # handleì—ì„œ @ ì œê±°
    clean_handle = handle.lstrip("@")
    url = f"https://www.youtube.com/@{clean_handle}"

    try:
        req = Request(url, headers={"User-Agent": "Mozilla/5.0"})
        with urlopen(req, timeout=15) as resp:
            html = resp.read().decode("utf-8", errors="replace")

        # channel_id ì¶”ì¶œ (ì—¬ëŸ¬ íŒ¨í„´ ì‹œë„)
        patterns = [
            r'"channelId"\s*:\s*"(UC[a-zA-Z0-9_-]{22})"',
            r'<meta\s+itemprop="channelId"\s+content="(UC[a-zA-Z0-9_-]{22})"',
            r'"externalId"\s*:\s*"(UC[a-zA-Z0-9_-]{22})"',
            r'/channel/(UC[a-zA-Z0-9_-]{22})',
        ]
        for pattern in patterns:
            match = re.search(pattern, html)
            if match:
                return match.group(1)

        print(f"  âš ï¸ ì±„ë„ IDë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {handle}", file=sys.stderr)
        return None
    except (URLError, HTTPError) as e:
        print(f"  âŒ ì±„ë„ í˜ì´ì§€ ì ‘ê·¼ ì‹¤íŒ¨ ({handle}): {e}", file=sys.stderr)
        return None


def fetch_rss_feed(channel_id: str) -> List[Dict]:
    """
    YouTube RSS í”¼ë“œì—ì„œ ì˜ìƒ ëª©ë¡ì„ ê°€ì ¸ì˜µë‹ˆë‹¤.
    """
    rss_url = f"https://www.youtube.com/feeds/videos.xml?channel_id={channel_id}"

    try:
        req = Request(rss_url, headers={"User-Agent": "Mozilla/5.0"})
        with urlopen(req, timeout=15) as resp:
            xml_data = resp.read().decode("utf-8", errors="replace")

        # XML íŒŒì‹±
        ns = {
            "atom": "http://www.w3.org/2005/Atom",
            "yt": "http://www.youtube.com/xml/schemas/2015",
            "media": "http://search.yahoo.com/mrss/",
        }

        root = ET.fromstring(xml_data)
        entries = root.findall("atom:entry", ns)

        videos = []
        for entry in entries:
            video_id_el = entry.find("yt:videoId", ns)
            title_el = entry.find("atom:title", ns)
            published_el = entry.find("atom:published", ns)
            link_el = entry.find("atom:link", ns)

            if video_id_el is None or title_el is None:
                continue

            video = {
                "video_id": video_id_el.text,
                "title": title_el.text,
                "published": published_el.text if published_el is not None else "",
                "url": f"https://www.youtube.com/watch?v={video_id_el.text}",
            }

            # media:groupì—ì„œ ì„¤ëª… ì¶”ì¶œ
            media_group = entry.find("media:group", ns)
            if media_group is not None:
                desc_el = media_group.find("media:description", ns)
                video["description"] = desc_el.text if desc_el is not None else ""
                thumb_el = media_group.find("media:thumbnail", ns)
                if thumb_el is not None:
                    video["thumbnail"] = thumb_el.get("url", "")

            videos.append(video)

        return videos
    except (URLError, HTTPError) as e:
        print(f"  âŒ RSS í”¼ë“œ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨ ({channel_id}): {e}", file=sys.stderr)
        return []
    except ET.ParseError as e:
        print(f"  âŒ RSS XML íŒŒì‹± ì‹¤íŒ¨ ({channel_id}): {e}", file=sys.stderr)
        return []


def filter_recent_videos(videos: List[Dict], hours: int = 24) -> List[Dict]:
    """
    ì§€ì •ëœ ì‹œê°„ ì´ë‚´ì— ê²Œì‹œëœ ì˜ìƒë§Œ í•„í„°ë§í•©ë‹ˆë‹¤.
    """
    cutoff = datetime.now(timezone.utc) - timedelta(hours=hours)
    recent = []

    for video in videos:
        pub_str = video.get("published", "")
        if not pub_str:
            continue
        try:
            # ISO 8601 í˜•ì‹ íŒŒì‹±: 2026-02-10T12:00:00+00:00
            pub_date = datetime.fromisoformat(pub_str.replace("Z", "+00:00"))
            if pub_date >= cutoff:
                video["published_dt"] = pub_date
                recent.append(video)
        except ValueError:
            continue

    return recent


def collect_videos(hours_lookback: int = None) -> List[Dict]:
    """
    ì „ì²´ ì±„ë„ì—ì„œ ìµœê·¼ ì˜ìƒì„ ìˆ˜ì§‘í•©ë‹ˆë‹¤.

    Returns:
        ì±„ë„ ì •ë³´ê°€ í¬í•¨ëœ ì˜ìƒ ë¦¬ìŠ¤íŠ¸
    """
    if hours_lookback is None:
        hours_lookback = HOURS_LOOKBACK

    all_videos = []
    print(f"\nğŸ“¡ ìœ íŠœë¸Œ ì˜ìƒ ìˆ˜ì§‘ ì‹œì‘... (ìµœê·¼ {hours_lookback}ì‹œê°„)")
    print(f"   ëŒ€ìƒ ì±„ë„: {len(YOUTUBE_CHANNELS)}ê°œ\n")

    for channel in YOUTUBE_CHANNELS:
        handle = channel["handle"]
        name = channel["name"]
        print(f"  ğŸ” [{name}] ì±„ë„ ê²€ìƒ‰ ì¤‘...")

        # ì±„ë„ ID ê°€ì ¸ì˜¤ê¸°
        channel_id = get_channel_id_from_handle(handle)
        if not channel_id:
            print(f"     âš ï¸ ê±´ë„ˆëœ€\n")
            continue

        print(f"     ì±„ë„ ID: {channel_id}")

        # RSS í”¼ë“œ ê°€ì ¸ì˜¤ê¸°
        all_feed_videos = fetch_rss_feed(channel_id)
        print(f"     í”¼ë“œ ì˜ìƒ ìˆ˜: {len(all_feed_videos)}")

        # ìµœê·¼ ì˜ìƒ í•„í„°ë§
        recent = filter_recent_videos(all_feed_videos, hours_lookback)
        print(f"     ìµœê·¼ {hours_lookback}ì‹œê°„ ì˜ìƒ: {len(recent)}ê°œ")

        for v in recent:
            v["channel_name"] = name
            v["channel_handle"] = handle
            v["channel_id"] = channel_id
            all_videos.append(v)

        print()

    print(f"âœ… ì´ ìˆ˜ì§‘ëœ ì˜ìƒ: {len(all_videos)}ê°œ\n")
    return all_videos


if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰ - ê¸°ë³¸ 24ì‹œê°„ ë˜ëŠ” ì¸ìë¡œ ì§€ì •
    import json

    hours = int(sys.argv[1]) if len(sys.argv) > 1 else HOURS_LOOKBACK
    videos = collect_videos(hours)

    if videos:
        print("\nğŸ“‹ ìˆ˜ì§‘ëœ ì˜ìƒ ëª©ë¡:")
        for i, v in enumerate(videos, 1):
            print(f"\n  {i}. [{v['channel_name']}] {v['title']}")
            print(f"     URL: {v['url']}")
            print(f"     ê²Œì‹œì¼: {v.get('published', 'N/A')}")
    else:
        print("\nâš ï¸ ìµœê·¼ì— ê²Œì‹œëœ ì˜ìƒì´ ì—†ìŠµë‹ˆë‹¤.")
        # ìµœê·¼ ì˜ìƒ ì—†ì–´ë„ í”¼ë“œ ì „ì²´ í™•ì¸
        print("\nğŸ“‹ ìµœê·¼ í”¼ë“œ í™•ì¸ (ì‹œê°„ ì œí•œ ì—†ì´):")
        all_vids = collect_videos(hours_lookback=999999)
        for i, v in enumerate(all_vids[:5], 1):
            print(f"  {i}. [{v['channel_name']}] {v['title']}")
            print(f"     ê²Œì‹œì¼: {v.get('published', 'N/A')}")
